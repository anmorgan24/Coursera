{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016bf241",
   "metadata": {},
   "source": [
    "# Selecting and Training a Model\n",
    "- Some best practice for building a machine learning model that is worthy of a production deployment\n",
    "- Model-centric AI development vs Data-centric AI development\n",
    "\n",
    "### Key Challenges\n",
    "AI system = Code (algorithm/model) + Data\n",
    "- A lot of times it will be more efficient to spend more of your time improving the data because the data usually has to be much more customized to your problem \n",
    "- When building a model, there are three key milestones that most models should aspire to accomplish:\n",
    "    - 1. Doing well on training set (usually measured by average training error).\n",
    "    - 2. Doing well on dev/test sets.\n",
    "    - 3. Doing well on business metrics/project goals.\n",
    "    \n",
    "### Why low average error isn't good enough\n",
    "#### Performance on disproportionately important examples\n",
    "- Web search example:\n",
    "    - **Informational and transactional queries:**\n",
    "        - \"Apple pie,\" \"Wireless data plan,\" \"Latest movies,\" \"Diwali festival.\"\n",
    "        - For informational and transactional queries, a web search engine wants to return the most relevant results, but users are willing to forgive (maybe) ranking the best result as number 2 or 3\n",
    "    - **Navigational queries:**\n",
    "        - \"Stanford,\" \"Reddit,\" \"YouTube.\"\n",
    "        - Here the user has a very clear desire to go to a particular place\n",
    "        - When a user has a very clear navigational intent, they will tend to be very unforgiving if the websearch engine does anything other than return the exact result\n",
    "        - A web search engines that doesn't return the best results will very quickly lose the trust of its users.\n",
    "        - So, navigational queries in this context are disproportionately important set of examples\n",
    "- The problem is that average test set accuracy tends to weight all examples equally, whereas in many cases some scenarios are disproportionately important. \n",
    "\n",
    "#### Performance on key slices of the dataset\n",
    "- **Example: ML for loan approval**\n",
    "    - Make sure not to discriminate by ethnicity, gender, location, language, or other protected attributes\n",
    "    - **Even if a learning algorithm for loan approval achieves high average test set accuracy, it would not be acceptable for production deployment if it exhibits an unacceptable level of bias or discrimination.**\n",
    "- **Example: Product recommendations from retailers**\n",
    "    - Be careful to treat fairly all major user-, retailer-, and product- categories/groups.\n",
    "    \n",
    "#### Rare classes\n",
    "- Skewed data distributions\n",
    "- Accuracy in rare classes\n",
    "\n",
    "### Establish a baseline\n",
    "- HLP (Human-Level Performance) is often a good point of comparison or a baseline that helps you decide where to focus your efforts\n",
    "\n",
    "<img src='img/1.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Unstructured and structured data\n",
    "- It turns out the best practices for establishing a baseline are quite different depending on whether you are using unstructured or structured data. \n",
    "- Because humans are so good at interpreting **unstructured data**, using HLP as a comparison is often a good way of establishing a baseline\n",
    "- In contrast, because humans are not as good at looking at **structured (tabular) data** and making predictions, HLP is generally a less useful baseline.\n",
    "- In general, ML best practices are typically very different depending on whether you're working with structured or unstructured data.\n",
    "\n",
    "<img src='img/2.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f92d61",
   "metadata": {},
   "source": [
    "#### Ways to establish a baseline\n",
    "- Human level performance (HLP) $\\rightarrow$ particularly for unstructured data problems. \n",
    "- Literature seach for state-of-the-art/open source (to see what others are able to accomplish).\n",
    "- Quick and dirty implementation \n",
    "- Performance of an older system (for example, if you already have a machine learning system running and are looking to replace it).\n",
    "\n",
    "**Baseline helps to indicate what might be possible. In some cases (such as HLP) it also gives a sense of what is irreducible error/Bayes' error.**\n",
    "\n",
    "#### Tips for getting started on modeling\n",
    "- Literature to see what's possible (courses, blogs, open-source projects).\n",
    "- Find open-source implementations if possible.\n",
    "- **A reasonable algorithm with good data will often outperform great a great algorithm with not so good data.**\n",
    "\n",
    "#### Deployment constraints when picking a model\n",
    "- Should you take into account deployment constraints (such as compute constraints) when picking a model?\n",
    "    - **Yes**, if baseline is already established and goal is to build and deploy.\n",
    "    - **No**, (or not necessarily), if purpose is to establish a baseline and determine waht is possible and might be worth pursuing.\n",
    "\n",
    "#### Sanity-check for code and algorithm\n",
    "- Try to overfit a small training dataset before training on a large one (especially if the output is a complex output).\n",
    "    - Example \\#1: Speech recognition\n",
    "    - Example \\#2: Image segmentation\n",
    "    - Example \\#3: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970712d",
   "metadata": {},
   "source": [
    "## Error Analysis and Performance Auditing\n",
    "- Error analysis within a Jupyter notebook or Excel spreadsheet is common, but there are also emerging MLOps tools to make this process much more accurate, efficient, and in some cases, automated.\n",
    "\n",
    "<img src='img/3.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- Error analysis is an iterative process\n",
    "\n",
    "<img src='img/4.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Useful metrics for each tag\n",
    "- What fraction of errors has that tag?\n",
    "- Of all the data with that tag, what fraction is misclassified?\n",
    "- What fraction of all the data has that tag?\n",
    "- How much room for improvement is there on data with that tag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173963df",
   "metadata": {},
   "source": [
    "### Prioritizing what to work on\n",
    "- How can we use the above-described tags to prioritize what we work on?\n",
    "- Beside \"Gap to HLP,\" one other useful performance to look at is **\"% of data\"**\n",
    "- Using these two columns together, we see that it may be best to prioritize either/both **Clean Speech** and/or **People Noise**.\n",
    "\n",
    "<img src='img/5.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Decide on most important categories to work on based on:\n",
    "   - How much room for improvement there is\n",
    "   - How frequently that category appears\n",
    "   - How easy (or not) it is to improve accuracy in a particular category\n",
    "   - How important it is to improve in that category\n",
    "   \n",
    "\n",
    "* There is no mathematical formula that will tell you what to work on, but by looking at these factors you'll hopefully be able to make some fruitful decisions. \n",
    "\n",
    "#### Adding/improving data for specific categories\n",
    "- For catgories you want to prioritize\n",
    "    - Collect more data\n",
    "    - Use data augmentation to get more data\n",
    "    - Improve label accuracy/data quality\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a030a6",
   "metadata": {},
   "source": [
    "### Skewed Datasets \n",
    "- Datasets where the ratio of positive to negative examples is very far from 50-50 are called **skewed data sets.**\n",
    "- **Confusion Matrix:** Actual vs. Predicted\n",
    "- **Precision:** $$\\frac{TP}{TP + FP}$$\n",
    "- **Recall:** $$\\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce1141",
   "metadata": {},
   "source": [
    "- The metrics of precision and recall are more useful than raw accuracy when it comes to evaluating the performance of learning algorithms on very skewed datasets\n",
    "\n",
    "#### Combining precision and recall- $F_1$ score\n",
    "- **The $F_1$ score is a common way of combining precision and recall that emphasizes whichever (of P or R) is worse.**\n",
    "- One intuition behind the $F_1$ score is that you want an algorithm to do well on both precision and recall, and if it does worse on either precision or recall, that's pretty bad.\n",
    "- The $F_1$ score is a **harmonic mean** (which is like taking an average, but putting an emphasis on whichever is the lower number).\n",
    "- **$F_1$ score:** $$\\frac{2}{\\frac{1}{P}+\\frac{1}{R}}$$\n",
    "\n",
    "<img src='img/6.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **Note that for your application, you may have a different weighting between precision and recall, so the $F_1$ score isn't the only way to combine precision and recall**, it's just one metric that's commonly used for many applications.\n",
    "- Precision and recall are not just useful for binary classification problems, but also for multi-class classification problems \n",
    "- You'll find in manufacturing that many factories will want high recall because you really don't want, for example, to let a phone go out that is defective. But if an algorithm has slightly lower precision that's okay because through a human re-examining the phone, they will hopefully figure out that the phone is actually okay. \n",
    "- By combining precision and recall with the $F_1$ score, this gives you a single evaluation metric for how well your algorithm is doing. \n",
    "- In the example below, if each of the four types of defects were very rare, then the accuracy would be extremely high for each category, even if the model was very bad at detecting any defects:\n",
    "\n",
    "<img src='img/7.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695f57e",
   "metadata": {},
   "source": [
    "### Performance Auditing\n",
    "- Even when your learning algorithm is doing well on accuracy or F1 score or some appropriate metric, it's often worth one last performance audit before you push it to production.\n",
    "#### Auditing framework \n",
    "- Check for accuracy, fairness/bias, and other problems \n",
    "    - 1. Brainstorm the ways the system might go wrong.\n",
    "        - Performance on subsets of data (e.g., ethnicity, gender).\n",
    "        - How common are certain errors (e.g., FP, FN).\n",
    "        - Performance on rare classes\n",
    "    - 2. Establish metrics to assess performance against these issues on appropriate **slices of data.**\n",
    "        - After establishing appropriate metrics, MLOps tools can also help trigger an automatic evaluation for each model to audit performance (e.g., TFMA)\n",
    "    - 3. Get business/product owner buy-in.\n",
    "\n",
    "#### Speech recognition example\n",
    "- 1. Brainstorm the ways the system might go wrong:\n",
    "    - Accuracy on different genders and ethnicities.\n",
    "    - Accuracy on different devices.\n",
    "    - Prevalence of rude mis-transcriptions.\n",
    "- 2. Establish metrics to assess performance against these issues on appropriate slices of data:\n",
    "    - Mean accuracy for different genders and major accents.\n",
    "    - Mean accuracy on different devices.\n",
    "    - Check for prevalence of offensive words in the output\n",
    "    - *The ways that a system might go wrong turns out to be very problem dependent: different industries, different tasks will have very different standards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f483d546",
   "metadata": {},
   "source": [
    "## Data iteration\n",
    "### Data-centric AI development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e76d7c",
   "metadata": {},
   "source": [
    "- **Model-centric view:** Take the data you have, and develop a model that does as well as possible on it\n",
    "    - Most academic research in AI is model-centric because the benchmarked dataset is a fixed quantity\n",
    "    - **Hold the data fixed and iteratively improve the code/model.**\n",
    "- **Data- centric view:** The quality of the data is paramount. Use tools to improve the data quality; this will allow multiple models to do well.\n",
    "    - Tools include error analysis and data augmentation\n",
    "    - **Hold the code fixed and iteratively improve the data.** \n",
    "    \n",
    "\n",
    "- There's a role for model-centric development and there's a role for data-centric development.\n",
    "\n",
    "### A useful picture of data augmentation\n",
    "\n",
    "<img src='img/8.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='img/9.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- It turns out that for unstructured data problems, pulling up one piece of the \"rubber sheet\" in the previous example is unlikely to cause a different part of the sheet to dip down far below. \n",
    "- Instead pulling up one point causes nearby points to be pulled up quite a lot, and far away points to maybe be pulled up a little bit (or if you're lucky, maybe more than a little bit). \n",
    "- And when you pull up part of the rubber sheet, the location of the biggest gap may shift to somewhere else and error analysis will tell you what is the location of this new biggest gap\n",
    "\n",
    "<img src='img/10.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530fa51",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "<img src='img/11.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **Goal:** is to create examples that your learning algorithm can learn from.\n",
    "    - Create **realistic examples** that (i) **the algorithm does poorly on**, but (ii) **humans (or other baseline do well on.**\n",
    "- **Checklist:** \n",
    "    - 1. Does it sound realistic?\n",
    "    - 2. Is the `x` $\\rightarrow$ `y` mapping clear? (e.g., can humans recognize speech?)\n",
    "    - 3. Is the algorithm currently doing poorly on it?\n",
    "    \n",
    "\n",
    "- **Taking a data-centric approach to AI development, sometimes it's useful to use a *data iteration loop* (rather than model iteration.**\n",
    "\n",
    "### Can adding data hurt?\n",
    "- For a lot of ML problems, training sets and dev and test set distribution strt out being reasonably similar. But, if you're using data augmentation, you're adding to specific parts of the training set such as adding lots of data with cafe noise. So now your training set may come from a very different distribution than the dev set and test set.\n",
    "- Is this going to hurt your learning algorithm? Usually the answer for unstructured data the answer is no (with some caveats)\n",
    "- For unstructured data problems, if:\n",
    "    - The model is large (low bias)\n",
    "    - The mapping `x` $\\rightarrow$ `y` is clear (e.g., given only the input `x`, humans can make accurate predictions).\n",
    "    - Then, **adding (accurately labeled) data rarely hurts accuracy.**\n",
    "    \n",
    "#### Photo OCR counterexample\n",
    "- Some images are truly ambiguous\n",
    "- Adding a lot of new \"I\"s (***especially ambiguous examples***) may skew the dataset and hurt performance\n",
    "- Because we know there are a lot more 1s than Is on house numbers, if the learning algorithm sees a picture like the one on the right, it would be much safer to guess that it is a 1.\n",
    "- This is an example of when the mapping of `x` $\\rightarrow$ `y` is not clear.\n",
    "- Just to be clear, this example is a pretty rare, almost corner case. It is quite unusually for data augmentation or adding data to hurt the performance of an ML algorithm.\n",
    "\n",
    "<img src='img/11.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "### Adding features\n",
    "- For many structured data problems, it turns out that creating brand new training examples is difficult, but what you can do is take existing training examples and figure out if there are additional useful features you can add to it. \n",
    "- For structured data problems, you usually have a fixed number of observations, and it's difficult if not impossible to add more. Instead, make new features from existing observations.\n",
    "\n",
    "<img src='img/12.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985aa84",
   "metadata": {},
   "source": [
    "#### Recommender Engines\n",
    "- Over the last several years, there’s been a trend in product recommendations of a shift from collaborative filtering approaches to what content based filtering approaches\n",
    "- Collaborative filtering —> tries to find users similar to you and then recommend items that those users liked\n",
    "- Content-based filtering --> Will tend to look at you as a person, and the description of the restaurant or the menu of the restaurant to see if that restaurant is a good match. \n",
    "\t- The advantage of content-based filtering is that, even if there is a new restaurant or a new product that hardly anyone else has liked, you can more quickly make good recommendations\n",
    "\t- “the cold start problem”: how do you recommend a brand new product — make sure that you capture good features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac616522",
   "metadata": {},
   "source": [
    "## Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826b282",
   "metadata": {},
   "source": [
    "- Rather than. worrying too much about exactly which experiment tracking framework to use, the number one thing to take away from this section is, do try to have some system, even if it's just a text file or just a spreadsheet for keeping track of your experiments, and include as much information as is convenient to inglude\n",
    "- **What to track:**\n",
    "    - Algorithm/code versioning\n",
    "    - Dataset used\n",
    "    - Hyperparameters\n",
    "    - Results\n",
    "- **Tracking tools:**\n",
    "    - Text files (does not scale well)\n",
    "    - Spreadsheets (scale much further, especially shared spreadsheets)\n",
    "    - Experiment tracking system\n",
    "- **Desirable features:**\n",
    "    - Information needed to replicate results?\n",
    "        - does your learning algorithm pull data off the internet? This can make experiments less reproducible\n",
    "    - Experiment results, ideally with summary metrics/analysis\n",
    "    - Perhaps also: Resource monitoring, visualization, model error analysis\n",
    "- The space of experiment tracking systems is still evolving rapidly and so there's a growing set of tools out there. But some examples include:\n",
    "    - W and B\n",
    "    - ***Comet***\n",
    "    - MLFlow\n",
    "    - Sage Maker Studio\n",
    "    - Landing.AI $\\rightarrow$ focuses on computer vision and manufacturing applications\n",
    "\n",
    "### From big data to good data\n",
    "- Try to ensure consistently high-quality data in all phases of the ML project lifecycle.\n",
    "- Good data:\n",
    "    - Covers important cases (good coverage of inputs `x`)\n",
    "    - Is defined consistently (definition of labels `y` is unambiguous)\n",
    "    - Has timely feedback from production data (distribution covers data drift and concept drift)\n",
    "    - Is sized appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e3b99",
   "metadata": {},
   "source": [
    "### More label ambiguity examples\n",
    "- A common application in many large companies is user ID merge\n",
    "    - **User ID merge:** When you have multiple data records that you think correspond to the same person and you want to merge these user records together.\n",
    "    - One scenario where this commonly occurs is when one company purchase or merges with another company and a user has accounts with each (often with not-identical information in each)\n",
    "    - One approach to the User ID merge problem is to take a supervised ML algorithm that takes as input two user data records and tries to output either one or zero based on whether it thinks these two are actually the sme user\n",
    "- Other examples with ambiguous ground truths: \n",
    "    - Is predicting if an account is a spam/fake/bot account\n",
    "    - Is an online purchase fraudulent?\n",
    "    - A job/resume website trying to predict whether a user is actively looking for a job or not\n",
    "    - Structuring text transcription\n",
    "\n",
    "\n",
    "#### Data definition questions\n",
    "- When defining the data for your learning algorithm, here are some important questions:\n",
    "    - What is the input, `x`?\n",
    "        - Lighting? Contrast? Resolution?\n",
    "        - What features need to be included?\n",
    "    - What is the target label, `y`?\n",
    "        - How can we ensure labelers give consistent labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7190c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f1316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114cb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec12439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ed8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d91ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fe8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6127eb",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
