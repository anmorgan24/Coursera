{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016bf241",
   "metadata": {},
   "source": [
    "# Selecting and Training a Model\n",
    "- Some best practice for building a machine learning model that is worthy of a production deployment\n",
    "- Model-centric AI development vs Data-centric AI development\n",
    "\n",
    "### Key Challenges\n",
    "AI system = Code (algorithm/model) + Data\n",
    "- A lot of times it will be more efficient to spend more of your time improving the data because the data usually has to be much more customized to your problem \n",
    "- When building a model, there are three key milestones that most models should aspire to accomplish:\n",
    "    - 1. Doing well on training set (usually measured by average training error).\n",
    "    - 2. Doing well on dev/test sets.\n",
    "    - 3. Doing well on business metrics/project goals.\n",
    "    \n",
    "### Why low average error isn't good enough\n",
    "#### Performance on disproportionately important examples\n",
    "- Web search example:\n",
    "    - **Informational and transactional queries:**\n",
    "        - \"Apple pie,\" \"Wireless data plan,\" \"Latest movies,\" \"Diwali festival.\"\n",
    "        - For informational and transactional queries, a web search engine wants to return the most relevant results, but users are willing to forgive (maybe) ranking the best result as number 2 or 3\n",
    "    - **Navigational queries:**\n",
    "        - \"Stanford,\" \"Reddit,\" \"YouTube.\"\n",
    "        - Here the user has a very clear desire to go to a particular place\n",
    "        - When a user has a very clear navigational intent, they will tend to be very unforgiving if the websearch engine does anything other than return the exact result\n",
    "        - A web search engines that doesn't return the best results will very quickly lose the trust of its users.\n",
    "        - So, navigational queries in this context are disproportionately important set of examples\n",
    "- The problem is that average test set accuracy tends to weight all examples equally, whereas in many cases some scenarios are disproportionately important. \n",
    "\n",
    "#### Performance on key slices of the dataset\n",
    "- **Example: ML for loan approval**\n",
    "    - Make sure not to discriminate by ethnicity, gender, location, language, or other protected attributes\n",
    "    - **Even if a learning algorithm for loan approval achieves high average test set accuracy, it would not be acceptable for production deployment if it exhibits an unacceptable level of bias or discrimination.**\n",
    "- **Example: Product recommendations from retailers**\n",
    "    - Be careful to treat fairly all major user-, retailer-, and product- categories/groups.\n",
    "    \n",
    "#### Rare classes\n",
    "- Skewed data distributions\n",
    "- Accuracy in rare classes\n",
    "\n",
    "### Establish a baseline\n",
    "- HLP (Human-Level Performance) is often a good point of comparison or a baseline that helps you decide where to focus your efforts\n",
    "\n",
    "<img src='img/1.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Unstructured and structured data\n",
    "- It turns out the best practices for establishing a baseline are quite different depending on whether you are using unstructured or structured data. \n",
    "- Because humans are so good at interpreting **unstructured data**, using HLP as a comparison is often a good way of establishing a baseline\n",
    "- In contrast, because humans are not as good at looking at **structured (tabular) data** and making predictions, HLP is generally a less useful baseline.\n",
    "- In general, ML best practices are typically very different depending on whether you're working with structured or unstructured data.\n",
    "\n",
    "<img src='img/2.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f92d61",
   "metadata": {},
   "source": [
    "#### Ways to establish a baseline\n",
    "- Human level performance (HLP) $\\rightarrow$ particularly for unstructured data problems. \n",
    "- Literature seach for state-of-the-art/open source (to see what others are able to accomplish).\n",
    "- Quick and dirty implementation \n",
    "- Performance of an older system (for example, if you already have a machine learning system running and are looking to replace it).\n",
    "\n",
    "**Baseline helps to indicate what might be possible. In some cases (such as HLP) it also gives a sense of what is irreducible error/Bayes' error.**\n",
    "\n",
    "#### Tips for getting started on modeling\n",
    "- Literature to see what's possible (courses, blogs, open-source projects).\n",
    "- Find open-source implementations if possible.\n",
    "- **A reasonable algorithm with good data will often outperform great a great algorithm with not so good data.**\n",
    "\n",
    "#### Deployment constraints when picking a model\n",
    "- Should you take into account deployment constraints (such as compute constraints) when picking a model?\n",
    "    - **Yes**, if baseline is already established and goal is to build and deploy.\n",
    "    - **No**, (or not necessarily), if purpose is to establish a baseline and determine waht is possible and might be worth pursuing.\n",
    "\n",
    "#### Sanity-check for code and algorithm\n",
    "- Try to overfit a small training dataset before training on a large one (especially if the output is a complex output).\n",
    "    - Example \\#1: Speech recognition\n",
    "    - Example \\#2: Image segmentation\n",
    "    - Example \\#3: Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970712d",
   "metadata": {},
   "source": [
    "## Error Analysis and Performance Auditing\n",
    "- Error analysis within a Jupyter notebook or Excel spreadsheet is common, but there are also emerging MLOps tools to make this process much more accurate, efficient, and in some cases, automated.\n",
    "\n",
    "<img src='img/3.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- Error analysis is an iterative process\n",
    "\n",
    "<img src='img/4.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Useful metrics for each tag\n",
    "- What fraction of errors has that tag?\n",
    "- Of all the data with that tag, what fraction is misclassified?\n",
    "- What fraction of all the data has that tag?\n",
    "- How much room for improvement is there on data with that tag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173963df",
   "metadata": {},
   "source": [
    "### Prioritizing what to work on\n",
    "- How can we use the above-described tags to prioritize what we work on?\n",
    "- Beside \"Gap to HLP,\" one other useful performance to look at is **\"% of data\"**\n",
    "- Using these two columns together, we see that it may be best to prioritize either/both **Clean Speech** and/or **People Noise**.\n",
    "\n",
    "<img src='img/5.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "#### Decide on most important categories to work on based on:\n",
    "   - How much room for improvement there is\n",
    "   - How frequently that category appears\n",
    "   - How easy (or not) it is to improve accuracy in a particular category\n",
    "   - How important it is to improve in that category\n",
    "   \n",
    "\n",
    "* There is no mathematical formula that will tell you what to work on, but by looking at these factors you'll hopefully be able to make some fruitful decisions. \n",
    "\n",
    "#### Adding/improving data for specific categories\n",
    "- For catgories you want to prioritize\n",
    "    - Collect more data\n",
    "    - Use data augmentation to get more data\n",
    "    - Improve label accuracy/data quality\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a030a6",
   "metadata": {},
   "source": [
    "### Skewed Datasets \n",
    "- Datasets where the ratio of positive to negative examples is very far from 50-50 are called **skewed data sets.**\n",
    "- **Confusion Matrix:** Actual vs. Predicted\n",
    "- **Precision:** $$\\frac{TP}{TP + FP}$$\n",
    "- **Recall:** $$\\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce1141",
   "metadata": {},
   "source": [
    "- The metrics of precision and recall are more useful than raw accuracy when it comes to evaluating the performance of learning algorithms on very skewed datasets\n",
    "\n",
    "#### Combining precision and recall- $F_1$ score\n",
    "- **The $F_1$ score is a common way of combining precision and recall that emphasizes whichever (of P or R) is worse.**\n",
    "- One intuition behind the $F_1$ score is that you want an algorithm to do well on both precision and recall, and if it does worse on either precision or recall, that's pretty bad.\n",
    "- The $F_1$ score is a **harmonic mean** (which is like taking an average, but putting an emphasis on whichever is the lower number).\n",
    "- **$F_1$ score:** $$\\frac{2}{\\frac{1}{P}+\\frac{1}{R}}$$\n",
    "\n",
    "<img src='img/6.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "- **Note that for your application, you may have a different weighting between precision and recall, so the $F_1$ score isn't the only way to combine precision and recall**, it's just one metric that's commonly used for many applications.\n",
    "- Precision and recall are not just useful for binary classification problems, but also for multi-class classification problems \n",
    "- You'll find in manufacturing that many factories will want high recall because you really don't want, for example, to let a phone go out that is defective. But if an algorithm has slightly lower precision that's okay because through a human re-examining the phone, they will hopefully figure out that the phone is actually okay. \n",
    "- By combining precision and recall with the $F_1$ score, this gives you a single evaluation metric for how well your algorithm is doing. \n",
    "- In the example below, if each of the four types of defects were very rare, then the accuracy would be extremely high for each category, even if the model was very bad at detecting any defects:\n",
    "\n",
    "<img src='img/7.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307019be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78358324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c3a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358ed34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186d0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a81aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fe8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a6127eb",
   "metadata": {},
   "source": [
    "<img src='img/x.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
